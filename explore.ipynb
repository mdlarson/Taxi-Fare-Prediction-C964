{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c4eb67-7aae-40b6-a58d-e58195843c2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Exploratory Data Analysis and Model Training for Taxi Fare Prediction\n",
    "\n",
    "This Jupyter Notebook supports initial data exploration and trains a Random Forest Regression \n",
    "model to predict taxi fares based on various input features. It includes steps for data extraction, \n",
    "preprocessing, feature engineering, model training, and evaluation.\n",
    "'''\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and Filter Data\n",
    "# We're only interested in trips to LaGuardia Airport,\n",
    "# so we'll create a new dataset for just those rows.\n",
    "# We calculate total cost, and discard outliers with negative costs.\n",
    "# We discard irrelevant columns to reduce dataset to minimal size.\n",
    "# This reduces the dataset from ~1GB combined to only ~9MB for 900k rows.\n",
    "# ---\n",
    "# Trip data source: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "# Dataset titles: \"High Volume For-Hire Vehicle Trip Records\"\n",
    "# Dataset URLs:\n",
    "# April 2024, https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-04.parquet\n",
    "# May 2024, https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-05.parquet\n",
    "# ---\n",
    "\n",
    "# File Paths\n",
    "input_files = [\n",
    "    'fhvhv_tripdata_2024-04.parquet',\n",
    "    'fhvhv_tripdata_2024-05.parquet'\n",
    "]\n",
    "output_file = 'filtered.parquet'\n",
    "borough_file = 'taxi_boroughs.csv' # output of parse_taxi_zones.py\n",
    "\n",
    "# Process Input Data\n",
    "# Read and process each input Parquet file\n",
    "dataframes = []\n",
    "for input_file in input_files:\n",
    "    raw = pd.read_parquet(input_file)\n",
    "\n",
    "    # Select features relevant for trip cost\n",
    "    columns_to_keep = ['pickup_datetime', 'PULocationID', 'DOLocationID']\n",
    "\n",
    "    # Select only trips to LGA – Taxi Zone 138, then drop the column\n",
    "    filtered_data = raw.loc[raw['DOLocationID'] == 138, columns_to_keep]\n",
    "    filtered_data = filtered_data.drop(columns=['DOLocationID'])\n",
    "\n",
    "    # Add column for sum of variable costs for each ride\n",
    "    filtered_data['total_cost'] = (\n",
    "        raw['base_passenger_fare'] +\n",
    "        raw['tolls'] +\n",
    "        raw['congestion_surcharge']\n",
    "    )\n",
    "\n",
    "    # Remove negative outliers\n",
    "    filtered_data = filtered_data[filtered_data['total_cost'] >= 0]\n",
    "\n",
    "    # Append processed dataframe to list\n",
    "    dataframes.append(filtered_data)\n",
    "\n",
    "# Combine input dataframes into one\n",
    "combined_data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Merge Borough Data\n",
    "# Read borough data\n",
    "boroughs = pd.read_csv(borough_file)\n",
    "# Merge borough names into dataset on matching Taxi Zone number\n",
    "combined_data = combined_data.merge(boroughs[['OBJECTID', 'borough']], left_on='PULocationID', right_on='OBJECTID').drop(columns=['OBJECTID'])\n",
    "\n",
    "# Save Working Dataset\n",
    "# Save to new working dataset for graphs and models to follow\n",
    "combined_data.to_parquet(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35717a89-4323-4db4-8fc4-84f3c0bac141",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Summarize Dataset\n",
    "data = pd.read_parquet(output_file)\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cbc79b-e040-4bab-9cb4-8f8fec7554ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Outliers\n",
    "\n",
    "# Calculate 25th and 75th percentile boundaries\n",
    "Q1 = data['total_cost'].quantile(0.25)\n",
    "Q3 = data['total_cost'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define cutoff point for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Remove outliers\n",
    "data = data[(data['total_cost'] >= lower_bound) & (data['total_cost'] <= upper_bound)]\n",
    "\n",
    "# Summarize again to check the results\n",
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a85749-912f-4231-938d-036094c98c7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform Data for Visualization and Modeling\n",
    "\n",
    "# Bin datetimes by hour of day\n",
    "data['pickup_datetime'] = pd.to_datetime(data['pickup_datetime']).dt.hour\n",
    "data.rename(columns={'pickup_datetime': 'hour_of_day'}, inplace=True)\n",
    "\n",
    "# Replace categorical Taxi Zone numbers and borough names with T/F dummy variables\n",
    "data = pd.get_dummies(data, columns=['PULocationID', 'borough'])\n",
    "\n",
    "# Save the list of feature_names for use in app.py\n",
    "feature_names = data.drop(\n",
    "    columns=['total_cost',\n",
    "             'borough_Bronx',\n",
    "             'borough_Brooklyn',\n",
    "             'borough_Manhattan',\n",
    "             'borough_Queens',\n",
    "             'borough_Staten Island']).columns.tolist()\n",
    "joblib.dump(feature_names, 'feature_names.pkl')\n",
    "\n",
    "# Sanity check\n",
    "print(data.head())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7c1529-db49-46f3-bd55-f5548f4721e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Data\n",
    "\n",
    "# Average cost by borough\n",
    "# Notable: Staten Island has highest base cost due to greatest distance\n",
    "# Notable: Queens has lowest base cost due to shortest distance\n",
    "plt.figure(figsize=(12, 6))\n",
    "graph = sns.catplot(x=\"total_cost\", y=\"borough\", order=['Bronx','Brooklyn','Manhattan','Queens','Staten Island'], kind=\"boxen\", height=6, aspect=2, data=combined_data)\n",
    "graph.fig.suptitle('Average Cost by Borough', y=1.03) # adjusting to avoid cutting off title\n",
    "graph.set_axis_labels(\"Average Fare\", \"Borough\")  # Set axis labels\n",
    "graph.savefig('static/catplot_avg_cost_by_borough.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d375303-b53e-48f9-b272-ca5e178845e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top pickup locations\n",
    "# Notable: had to use a log scale to see Staten Island\n",
    "# Notable: Staten Island count reduced due to exclusion of pricy outliers\n",
    "borough_dummies = ['borough_Bronx', 'borough_Brooklyn', 'borough_Manhattan', 'borough_Queens', 'borough_Staten Island']\n",
    "\n",
    "pickup_counts = data[borough_dummies].sum().sort_values(ascending=False).head(20)\n",
    "pickup_counts.index = pickup_counts.index.str.replace('borough_', '') # rename the columns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(x=pickup_counts.values, y=pickup_counts.index)\n",
    "plt.xscale('log')\n",
    "plt.title('Top Pickup Locations')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Pickup Location')\n",
    "ax.bar_label(ax.containers[0], fontsize=8)\n",
    "plt.savefig('static/barplot_top_pickup_locations.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b958595-2d66-43da-9b47-3523c8ef3053",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribution of total cost\n",
    "# Notable: dip at ~$15-25 range, possible impact of tolls on similar-distance trips\n",
    "# Notable: very, very long tail before removing outliers\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data['total_cost'], bins=50, kde=True)\n",
    "plt.title('Distribution of Fares')\n",
    "plt.xlabel('Total Cost')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('static/histplot_distribution_of_fares.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135f2d54-fb77-462c-9ea1-0fd4a922b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total cost by hour of day\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='hour_of_day', y='total_cost', data=data)\n",
    "plt.title('Cost by Hour of Day')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Cost')\n",
    "plt.savefig('static/boxplot_cost_by_hour_of_day.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf57c3-9eb1-4d68-bc0b-9b5763f0a6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average cost by pickup hour\n",
    "# Notable: greater variance at night\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='hour_of_day', y='total_cost', data=data)\n",
    "plt.title('Average Cost by Pickup Hour')\n",
    "plt.xlabel('Pickup Hour')\n",
    "plt.ylabel('Cost')\n",
    "plt.xticks(rotation=0)\n",
    "plt.savefig('static/barplot_avg_cost_by_pickup_hour.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8861b9c-ae97-4106-b50e-25cb3781db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Two Models\n",
    "\n",
    "# We're predicting fares at two levels of granularity: borough and neighborhood (Taxi Zone).\n",
    "# We'll use boroughs for our simplest baseline model.\n",
    "# Then compare with the neighborhood model to see if more features improve predictive power.\n",
    "\n",
    "# Before we create models, we need to make sure each is using only the relevant features.\n",
    "# We'll identify the columns that need to be dropped.\n",
    "borough_columns = [col for col in data.columns if col.startswith('borough')]\n",
    "neighborhood_columns = [col for col in data.columns if col.startswith('PULocationID')]\n",
    "\n",
    "# Borough data should exclude the neighborhoods\n",
    "# Neighborhood data should exclude the boroughs\n",
    "borough_data = data.drop(columns=neighborhood_columns)\n",
    "neighborhood_data = data.drop(columns=borough_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3975756-3553-4682-9ada-ad00299f45cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Evaluate Borough Model\n",
    "\n",
    "# Prepare data\n",
    "X_borough = borough_data.drop(columns=['total_cost'])\n",
    "y_borough = borough_data['total_cost']\n",
    "\n",
    "# Split data\n",
    "X_train_borough, X_test_borough, y_train_borough, y_test_borough = train_test_split(X_borough, y_borough, test_size=0.2, random_state=38)\n",
    "\n",
    "# Display training set to verify splits\n",
    "print(X_train_borough.head())\n",
    "\n",
    "# Train the model\n",
    "rf_borough = RandomForestRegressor(random_state=38)\n",
    "rf_borough.fit(X_train_borough, y_train_borough)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_borough = rf_borough.predict(X_test_borough)\n",
    "mae_borough = mean_absolute_error(y_test_borough, y_pred_borough)\n",
    "mse_borough = mean_squared_error(y_test_borough, y_pred_borough)\n",
    "r2_borough = r2_score(y_test_borough, y_pred_borough)\n",
    "\n",
    "# Print performance summary\n",
    "print(f\"Borough Model - MAE: {mae_borough}, MSE: {mse_borough}, R²: {r2_borough}\")\n",
    "\n",
    "# Performance Results for Borough Model\n",
    "# Model 1, Initial Run\n",
    "# MAE: 11.858519352878245, MSE: 268.929452004129, R²: 0.442588901962921\n",
    "# Model 2, After Removing Outliers\n",
    "# MAE: 10.077229327066409, MSE: 167.0615420693796, R²: 0.5103575632105499\n",
    "# Model 3, After Combining Two Months' Parquet Files\n",
    "# MAE: 10.49845986493553, MSE: 183.0367370351296, R²: 0.510364551383309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eac71c-0724-4fee-af14-f1bfa0161b69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train and Evaluate Neighborhood Model\n",
    "\n",
    "# Prepare data\n",
    "X_neighborhood = neighborhood_data.drop(columns=['total_cost'])\n",
    "y_neighborhood = neighborhood_data['total_cost']\n",
    "\n",
    "# Split data\n",
    "X_train_neighborhood, X_test_neighborhood, y_train_neighborhood, y_test_neighborhood = train_test_split(X_neighborhood, y_neighborhood, test_size=0.2, random_state=38)\n",
    "\n",
    "# Display training set to verify splits\n",
    "print(X_train_neighborhood.head())\n",
    "\n",
    "# Train the model\n",
    "rf_neighborhood = RandomForestRegressor(random_state=38)\n",
    "rf_neighborhood.fit(X_train_neighborhood, y_train_neighborhood)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_neighborhood = rf_neighborhood.predict(X_test_neighborhood)\n",
    "mae_neighborhood = mean_absolute_error(y_test_neighborhood, y_pred_neighborhood)\n",
    "mse_neighborhood = mean_squared_error(y_test_neighborhood, y_pred_neighborhood)\n",
    "r2_neighborhood = r2_score(y_test_neighborhood, y_pred_neighborhood)\n",
    "\n",
    "# Print performance summary\n",
    "print(f\"Neighborhood Model - MAE: {mae_neighborhood}, MSE: {mse_neighborhood}, R²: {r2_neighborhood}\")\n",
    "\n",
    "# Performance Results for Neighborhood Model\n",
    "# Model 1, Initial Run\n",
    "# MAE: 9.033939139834594, MSE: 186.04478396046923, R²: 0.6143842686672932\n",
    "# Model 2, After Removing Outliers\n",
    "# MAE: 7.367526755657218, MSE: 102.62094150209452, R²: 0.6992271995080372\n",
    "# Model 3, After Combining Two Months' Parquet Files\n",
    "# MAE: 7.700498524423761, MSE: 113.15572572538952, R²: 0.6973009056730255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551cff1-937e-47b5-9144-42b0c3d8a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Trained Model\n",
    "model_filename = 'modelv3.pkl'\n",
    "joblib.dump(rf_neighborhood, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46a710c-dbd3-48eb-ae45-3152031146f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
